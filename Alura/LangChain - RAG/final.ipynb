{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce7521f3",
   "metadata": {},
   "source": [
    "# Tudo junto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ae825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_milvus import Milvus\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd3da9c",
   "metadata": {},
   "source": [
    "## Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd04220",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs = DirectoryLoader(\"documentos\", glob=\"*.pdf\").load()\n",
    "len(pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb631183",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = \"BAAI/bge-m3\"\n",
    "embeddings_tokenizer = AutoTokenizer.from_pretrained(embeddings_model)\n",
    "\n",
    "splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=embeddings_tokenizer,\n",
    "    chunk_size=1250, chunk_overlap=150\n",
    ")\n",
    "\n",
    "pedacos = splitter.split_documents(pdfs)\n",
    "len(pedacos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e411e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = OllamaEmbeddings(model=\"bge-m3:567m\")\n",
    "\n",
    "URI = \"./milvus_example.db\"\n",
    "\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings_model,\n",
    "    connection_args={\"uri\": URI},\n",
    "    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d7e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(pedacos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d06874",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_model = OllamaLLM(model=\"gemma3:1b\")\n",
    "\n",
    "# multi_query_prompt_template = \"\"\"Você é um assistente de modelo de linguagem de IA. Sua tarefa é gerar cinco\n",
    "# versões diferentes da pergunta do usuário para recuperar documentos relevantes de um banco de dados vetorial.\n",
    "# Ao gerar múltiplas perspectivas sobre a pergunta do usuário, seu objetivo é ajudar\n",
    "# o usuário a superar algumas das limitações da busca por similaridade baseada em distância.\n",
    "# Forneça estas perguntas alternativas separadas por quebras de linha.\n",
    "# Responda apenas com os textos das perguntas, sem introdução ou comentários finais. Não coloque bullets ou numeros nas linhas.\n",
    "# Pergunta original: {question}\n",
    "# Perguntas:\"\"\"\n",
    "\n",
    "# multi_query_prompt = PromptTemplate.from_template(multi_query_prompt_template)\n",
    "\n",
    "# multi_query_chain = multi_query_prompt | query_model | CommaSeparatedListOutputParser()\n",
    "\n",
    "# multi_query_retriever = MultiQueryRetriever(\n",
    "#     retriever=vector_store.as_retriever(), llm_chain=multi_query_chain, \n",
    "# )\n",
    "\n",
    "hyde_prompt_template = \"\"\"\n",
    "Escreva uma frase que possa responder à pergunta apresentada. Não adicione mais nada.\n",
    "Pergunta: {query}\n",
    "Frase:\n",
    "\"\"\"\n",
    "\n",
    "hyde_prompt = PromptTemplate.from_template(hyde_prompt_template)\n",
    "# Next, build the HyDE chain:\n",
    "hyde_chain = hyde_prompt | query_model | StrOutputParser() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fd1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_model = OllamaLLM(model=\"granite3.3:8b\")\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [ \n",
    "        (\"system\", \"Responda usando exclusivamente os conteúdo fornecido. Seja breve na resposta com no máximo 100 palavras.\\n\\nContexto:\\n{contexto}\"),\n",
    "        (\"human\", \"{query}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "cadeia = (\n",
    "    {\n",
    "        \"contexto\": {\"query\": RunnablePassthrough()} | hyde_chain | vector_store.as_retriever(), \n",
    "        \"query\": RunnablePassthrough(),#B\n",
    "    }\n",
    "    | rag_prompt \n",
    "    | question_model \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53032855",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadeia.invoke(\"Como fazer um seguro viagem?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd95f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadeia.invoke(\"O que fazer se tiver meu cartão Gold roubado?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2733693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(hyde_chain.invoke(state[\"question\"]))\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = rag_prompt.invoke({\"query\": state[\"question\"], \"contexto\": docs_content})\n",
    "    response = question_model.invoke(messages)\n",
    "    return {\"answer\": response}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5214d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"question\": \"Quais os beneficios de um cartão platinum?\" })['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb756e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define your desired data structure.\n",
    "class QandA(BaseModel):\n",
    "    question: str = Field(description=\"question\")\n",
    "    answer: str = Field(description=\"answer to the question\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "modelo = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=QandA)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Com base no conteúdo fornecido, crie duas perguntas, uma em cada linha, sem marcador ou numeração.\\n\\nConteúdo: {content}\\n\",\n",
    "    input_variables=[\"content\"],\n",
    "    # partial_variables={\"format_instructions\": QandA.model_json_schema()},\n",
    ")\n",
    "\n",
    "chain = prompt | modelo | StrOutputParser()\n",
    "\n",
    "# perguntas = chain.batch([p.page_content for p in pedacos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d401f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "perguntas_respostas = []\n",
    "\n",
    "resposta_prompt = PromptTemplate(\n",
    "    template=\"Com base no conteúdo fornecido, responda a pergunda em no máximo duas frases.\\n\\nConteúdo: {content}\\n\\nPergunta: {pergunta}\\n\\nResposta:\",\n",
    "    input_variables=[\"content\", \"pergunta\"],\n",
    "    # partial_variables={\"format_instructions\": QandA.model_json_schema()},\n",
    ")\n",
    "\n",
    "resposta_chain = resposta_prompt | modelo | StrOutputParser()\n",
    "\n",
    "for i in range(len(pedacos)):\n",
    "    for pergunta in perguntas[i].split(\"\\n\"):\n",
    "        print(pergunta)\n",
    "        perguntas_respostas.append({\"pergunta\": pergunta.strip(), \"resposta\": resposta_chain.invoke({\"content\": pedacos[i].page_content, \"pergunta\": pergunta.strip()})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a96227",
   "metadata": {},
   "outputs": [],
   "source": [
    "perguntas_respostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"test_qa.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(perguntas_respostas, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "\n",
    "# Initialize the LLM for generating Q&A pairs\n",
    "example_gen_chain = QAGenerateChain.from_llm(modelo)\n",
    "\n",
    "# Generate Q&A pairs from the documents\n",
    "# The input to apply_and_parse should be a list of dictionaries, \n",
    "# where each dictionary contains a 'doc' key with the text content.\n",
    "new_examples = example_gen_chain.apply_and_parse(\n",
    "    [{\"doc\": t.page_content} for t in pedacos]\n",
    ")\n",
    "\n",
    "# Print the generated examples\n",
    "# for example in new_examples:\n",
    "#     print(f\"Query: {example['query']}\")\n",
    "#     print(f\"Answer: {example['answer']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0d6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"qa_pairs.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(new_examples, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbe614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open(\"qa_pairs.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    new_examples = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab85925",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"contexto\": {\"query\": RunnablePassthrough()} | hyde_chain | vector_store.as_retriever(), \n",
    "        \"query\": RunnablePassthrough(),#B\n",
    "    }\n",
    "    | rag_prompt \n",
    "    | modelo \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "eval_data = [{\"query\": pair[\"qa_pairs\"][\"query\"], \"answer\": pair[\"qa_pairs\"][\"answer\"] } for pair in new_examples]\n",
    "\n",
    "eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108eae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = chain.batch(eval_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a0430",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8274fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain\n",
    "eval_chain = QAEvalChain.from_llm(modelo)\n",
    "graded_outputs = eval_chain.evaluate(eval_data, [ {\"result\": p } for p in predictions ])\n",
    "graded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed78e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects = 0\n",
    "for i, eg in enumerate(eval_data):\n",
    "    # print(f\"Example {i}:\")\n",
    "    # print(\"Question: \" + eval_data[i]['query'])\n",
    "    # print(\"Real Answer: \" + eval_data[i]['answer'])\n",
    "    # print(\"Predicted Answer: \" + predictions[i])\n",
    "    # (\"Predicted Grade: \" + graded_outputs[i]['results'].split(\"\\n\")[-1].split(\":\")[-1].strip())\n",
    "    corrects = corrects + (1 if graded_outputs[i]['results'].split(\"\\n\")[-1].split(\":\")[-1].strip() == \"CORRECT\" else 0)\n",
    "    # print()\n",
    "    \n",
    "corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab534cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects / len(eval_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
